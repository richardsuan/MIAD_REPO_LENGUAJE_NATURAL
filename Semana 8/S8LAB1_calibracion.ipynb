{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JobAyJx67Jrm"
   },
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n14bqXe7NO8"
   },
   "source": [
    "# Calibración de parámetros de redes neuronales\n",
    "En este notebook aprenderá a calibrar parámetros de redes neuronales con dos diferentes metodologías usando la librería Keras y sklearn.model_selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJSdZveh7lg-"
   },
   "source": [
    "## Instrucciones Generales\n",
    "\n",
    "Este notebook esta compuesto por tres secciones. En la primera sección, usted beberá calibrar una red neuronal para predecir el precio de una casa con el set de datos Boston Housing Data y con el método de busqueda por cuadrícula (Grid Search). En la segunda, se usará el mismo dataset pero usted beberá calibrar la red neuronal con el método de busqueda aleatoria (Randomized Search), lo que le permitirá identificar las ventajas y diferencias entre los dos métodos. Para conocer más detalles de la base, puede ingresar al siguiente [vínculo](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/).\n",
    "   \n",
    "Para realizar la actividad, solo siga las indicaciones asociadas a cada celda del notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOGLBrdc94q8"
   },
   "source": [
    "## Importar base de datos y librerías"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T02:47:33.920694Z",
     "start_time": "2024-05-25T02:47:33.913774Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kbt-2z_VBZ-p",
    "ExecuteTime": {
     "end_time": "2024-05-25T02:47:34.689392Z",
     "start_time": "2024-05-25T02:47:33.922438Z"
    }
   },
   "source": [
    "# Importación librerías\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Importación librerías\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_boston\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/sklearn/datasets/__init__.py:157\u001B[0m, in \u001B[0;36m__getattr__\u001B[0;34m(name)\u001B[0m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mload_boston\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    109\u001B[0m     msg \u001B[38;5;241m=\u001B[39m textwrap\u001B[38;5;241m.\u001B[39mdedent(\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;124m        `load_boston` has been removed from scikit-learn since version 1.2.\u001B[39m\n\u001B[1;32m    111\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;124m        <https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\u001B[39m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;124m        \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[0;32m--> 157\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg)\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    159\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m()[name]\n",
      "\u001B[0;31mImportError\u001B[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5p1mbuz_BqR0",
    "outputId": "f3c689ba-271f-44ca-8fdf-d95708cca34a",
    "ExecuteTime": {
     "end_time": "2024-05-25T02:47:34.690831Z",
     "start_time": "2024-05-25T02:47:34.690705Z"
    }
   },
   "source": [
    "from sklearn.datasets import load_boston\n",
    "# Carga de datos de la librería sklearn\n",
    "boston_dataset = load_boston()\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJhBGPZZ99ME"
   },
   "source": [
    "## Definición de variables predictoras  y de interés"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YF-qHy3cBqKT"
   },
   "source": [
    "# Definición de variables predictoras (X)\n",
    "X = boston.drop(boston.columns[-1],axis=1)\n",
    "# Definición de variable de interés (y)\n",
    "Y = pd.DataFrame(np.array(boston_dataset.target), columns=['labels'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ekbRHWCMBqHc"
   },
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y) en set de entrenamiento y test usando la función train_test_split\n",
    "X_train, X_test , Y_train, Y_test = train_test_split(X,Y, test_size=0.3 ,random_state=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uLls8jzgBvFn"
   },
   "source": [
    "# Normalización de variables predictoras (X) con la función StandardScaler\n",
    "\n",
    "# Definición de la función StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transformación de los set de entrenamiento y test\n",
    "X_train = pd.DataFrame(data=scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test = pd.DataFrame(data=scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9v1UsRfBvDB",
    "outputId": "70506311-a640-44ce-9cd3-940b3510e246"
   },
   "source": [
    "# Definición de dimensiones de salida (variables de interés)\n",
    "output_var = Y_train.shape[1]\n",
    "print(output_var, ' output variables')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3qYY0pQBvAv",
    "outputId": "9b39cdc3-eeb0-4bbc-ce16-f8b606e000b0",
    "ExecuteTime": {
     "end_time": "2024-05-25T02:47:35.295111Z",
     "start_time": "2024-05-25T02:47:35.262798Z"
    }
   },
   "source": [
    "# Definición de dimensiones de entrada (variables predictoras)\n",
    "dims = X_train.shape[1]\n",
    "print(dims, 'input variables')"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Definición de dimensiones de entrada (variables predictoras)\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m dims \u001B[38;5;241m=\u001B[39m \u001B[43mX_train\u001B[49m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(dims, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput variables\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TeUQHtBD--6O",
    "ExecuteTime": {
     "end_time": "2024-05-25T02:47:35.622274Z",
     "start_time": "2024-05-25T02:47:35.602514Z"
    }
   },
   "source": [
    "# Separación de datos de entrenamiento para considerar un set de validación durante entrenamiento\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.15, random_state=42)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Separación de datos de entrenamiento para considerar un set de validación durante entrenamiento\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m X_train, X_val, Y_train, Y_val \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m(X_train, Y_train, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.15\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monq1M5YCY1i"
   },
   "source": [
    "## Red Neuronal\n",
    "\n",
    "En esta sección se construirá un modelo de red neuronal mediante una función (*nn_model_params*) que parametriza los diferentes parámetros a calibrar. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19gHI5k3SUI-",
    "outputId": "55aa4ed9-897c-4eb5-8846-cc56dba47f3b",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-05-25T02:47:37.167244Z",
     "start_time": "2024-05-25T02:47:37.164356Z"
    }
   },
   "source": [
    "# Para correr esta sección instale la librería livelossplot\n",
    "pip install livelossplot"
   ],
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (54256859.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[5], line 2\u001B[0;36m\u001B[0m\n\u001B[0;31m    pip install livelossplot\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aaOKn_dwCZKq",
    "ExecuteTime": {
     "end_time": "2024-05-25T02:47:39.914114Z",
     "start_time": "2024-05-25T02:47:37.689238Z"
    }
   },
   "source": [
    "# Importación librerías\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from livelossplot import PlotLossesKeras"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 21:47:37.876041: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-24 21:47:37.876399: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-24 21:47:37.878765: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-24 21:47:37.908035: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-24 21:47:38.820587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Importación librerías\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential, Model\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dense, Input\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m metrics\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/__init__.py:21\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mDetailed documentation and user guides are available at\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m[keras.io](https://keras.io).\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minput_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Input\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequential\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/models/__init__.py:18\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Functional\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequential\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/engine/functional.py:34\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m input_spec\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m node \u001B[38;5;28;01mas\u001B[39;00m node_module\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m training \u001B[38;5;28;01mas\u001B[39;00m training_lib\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m training_utils\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m serialization\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:40\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m optimizer_v1\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pickle_utils\n\u001B[0;32m---> 40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m saving_api\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m saving_lib\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m serialization_lib\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/saving/saving_api.py:24\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtf_export\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras_export\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m saving_lib\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m save \u001B[38;5;28;01mas\u001B[39;00m legacy_sm_saving_lib\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m io_utils\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/saving/legacy/save.py:27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m serialization\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaved_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load \u001B[38;5;28;01mas\u001B[39;00m saved_model_load\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaved_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_context\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaved_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m save \u001B[38;5;28;01mas\u001B[39;00m saved_model_save\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaved_model\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras_option_scope\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/saving/legacy/saved_model/load_context.py:68\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns whether under a load context.\"\"\"\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _load_context\u001B[38;5;241m.\u001B[39min_load_context()\n\u001B[0;32m---> 68\u001B[0m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__internal__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mregister_load_context_function\u001B[49m(in_load_context)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMfssAh9ClOS"
   },
   "outputs": [],
   "source": [
    "# Definición de función que crea una red neuronal a partir de diferentes parámetros (nn_model_params)\n",
    "# En esta función se consideran 7 parámetos a calibrar, sin embargo se pueden agregar o quitar tantos como lo consideren pertinente\n",
    "def nn_model_params(optimizer ,\n",
    "                    neurons,\n",
    "                    batch_size,\n",
    "                    epochs,\n",
    "                    activation,\n",
    "                    patience,\n",
    "                    loss):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    # Definición red neuronal con la función Sequential()\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Definición de las capas de la red con el número de neuronas y la función de activación definidos en la función nn_model_params\n",
    "    model.add(Dense(neurons, input_shape=(dims,), activation=activation))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(output_var, activation=activation))\n",
    "\n",
    "    # Definición de función de perdida con parámetros definidos en la función nn_model_params\n",
    "    model.compile(optimizer = optimizer, loss=loss)\n",
    "    \n",
    "    # Definición de la función EarlyStopping con parámetro definido en la función nn_model_params\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience = patience)\n",
    "\n",
    "    # Entrenamiento de la red neuronal con parámetros definidos en la función nn_model_params\n",
    "    model.fit(X_train, Y_train,\n",
    "              validation_data = (X_val, Y_val),\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              callbacks=[early_stopping, PlotLossesKeras()],\n",
    "              verbose=True\n",
    "              )\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMahbr2O7DcX"
   },
   "outputs": [],
   "source": [
    "# Definición de parámetros y sus valores sobre los que se va a calibrar\n",
    "nn_params = {\n",
    "    'optimizer': ['adam','sgd'],\n",
    "    'activation': ['relu'],\n",
    "    'batch_size': [64,128],\n",
    "    'neurons':[64,256],\n",
    "    'epochs':[20,50],\n",
    "    'patience':[2,5],\n",
    "    'loss':['mean_squared_error']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMxeJIDU-Ex7"
   },
   "source": [
    "## Método busqueda por cuadrícula (Grid Search)\n",
    "\n",
    "La búsqueda por cuadrícula es un método de calibración de parámetros donde se considera exhaustivamente todas las combinaciones de parámetros de un conjunto determinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "id": "60H_vZzrColz",
    "outputId": "b1bdbc86-fa94-4425-8b1a-572ff814ea9f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definición de red neuronal usando el wrapper KerasRegressor y usando como argumento build_fn en la función nn_model_params\n",
    "# para más detalles del wrapper puede ingresar al siguiente link https://faroit.com/keras-docs/1.2.2/scikit-learn-api/\n",
    "nn_model = KerasRegressor(build_fn=nn_model_params, verbose=0)\n",
    "\n",
    "# Definición método GridSearch para la calibración de parámetros definidos en nn_params\n",
    "gs = GridSearchCV(nn_model, nn_params, cv=3)\n",
    "gs.fit(X_train, Y_train)\n",
    "\n",
    "print('Los mejores parametros segun Grid Search:', gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5XDq3VD-lht"
   },
   "source": [
    "## Método busqueda aleatoria (Randomized Search)\n",
    "\n",
    "La busqueda aleatoria es un método de calibración de parámetros donde se consideran combinaciones aleatorias de parámetros de un conjunto determinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "id": "69ZrV0OKCTlq",
    "outputId": "98052070-3f7b-4c84-f315-6c81b17d4462"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Definición de red neuronal usando el wrapper KerasRegressor y usando como argumento build_fn en la función nn_model_params\n",
    "nn_model = KerasRegressor(build_fn=nn_model_params, verbose=0)\n",
    "\n",
    "# Definición método GridSearch para la calibración de parámetros definidos en nn_params\n",
    "rs = RandomizedSearchCV(nn_model, param_distributions=nn_params, n_iter=5, cv=3)\n",
    "rs.fit(X_train, Y_train)\n",
    "\n",
    "print('Los mejores parametros segun Randomnized Search:', rs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_opRoBB7RYfu"
   },
   "source": [
    "## Comparación de resultados\n",
    "En esta última sección se comparará el desempeño de la red neuronal en el set de test con la métrica del MSE, dados los parámetros calibrados que se obtuvieron en cada uno de los métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "jLfXdEiM1tzx",
    "outputId": "d9528a6e-9b15-49b6-ab0f-02c330adf963"
   },
   "outputs": [],
   "source": [
    "# Desempeño de modelo con los parametros de Grid Search\n",
    "model = nn_model_params(optimizer = 'adam',\n",
    "                        neurons=256,\n",
    "                        batch_size=64,\n",
    "                        epochs=50,\n",
    "                        activation='relu',\n",
    "                        patience=5,\n",
    "                        loss='mean_squared_error')\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Valor de \", model.metrics_names[0] ,\" o MSE dados los parámetros de Grid Search:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "kWzmoL6zNb5Y",
    "outputId": "d31af876-bd87-4d69-9c10-c85c2cf27836"
   },
   "outputs": [],
   "source": [
    "# Desempeño de modelo con los parametros de Randomized Search\n",
    "model = nn_model_params(optimizer = 'adam',\n",
    "                        neurons=256,\n",
    "                        batch_size=64,\n",
    "                        epochs=20,\n",
    "                        activation='relu',\n",
    "                        patience=5,\n",
    "                        loss='mean_squared_error')\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Valor de \", model.metrics_names[0] ,\" o MSE dados los parámetros de Randomized Search:\", scores)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gOGLBrdc94q8",
    "gJhBGPZZ99ME"
   ],
   "name": "S8LAB1_calibracion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
